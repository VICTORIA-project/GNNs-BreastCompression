{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing the output of the LODO experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Working Directory to the Notebook's Path\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "notebook_path = Path().resolve()\n",
    "os.chdir(notebook_path)\n",
    "\n",
    "# Confirm the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import vtk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the csv files \n",
    "\n",
    "*The output of the PhysGNN is shuffled, therefore we need to correct the shufflign and the reorder the csv files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unshuffled ground truth csv file as reference\n",
    "\n",
    "input_file_path = 'LODO/final_step/a/output_displacement.csv'  \n",
    "output_file_path = 'process-compressed/reference.csv'  \n",
    "\n",
    "# Step 1: Read the CSV file without headers\n",
    "df = pd.read_csv(input_file_path, header=None)\n",
    "\n",
    "# Step 2: Rename the columns to 'x', 'y', 'z'\n",
    "df.columns = ['x', 'y', 'z']\n",
    "\n",
    "# Step 3: Add an 'id' column starting from 1\n",
    "df.insert(0, 'id', range(1, len(df) + 1))\n",
    "\n",
    "# Step 4: Save the updated DataFrame back to a CSV file\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV file saved as '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shuffled ground truth csv file as shuffled\n",
    "\n",
    "input_file_path = 'GNNs-BreastCompression/LODO/Results_LODO/csv/actual_config3.csv'  \n",
    "output_file_path = 'process-compressed/shuffled.csv'  \n",
    "\n",
    "# Step 1: Read the CSV file (with current header)\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Step 2: Rename the existing columns to 'id', 'x', 'y', 'z'\n",
    "df.columns = ['id', 'x', 'y', 'z']\n",
    "\n",
    "# Step 3: Update the 'id' column to be a sequential range starting from 1\n",
    "df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "# Step 4: Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV file saved as '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted displacement csv file as prediction\n",
    "\n",
    "input_file_path = 'GNNs-BreastCompression/LODO/Results_LODO/csv/prediction_config3.csv' \n",
    "output_file_path = 'process-compressed/prediction.csv'  \n",
    "\n",
    "# Step 1: Read the CSV file (with current header)\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Step 2: Rename the existing columns to 'id', 'x', 'y', 'z'\n",
    "df.columns = ['id', 'x', 'y', 'z']\n",
    "\n",
    "# Step 3: Update the 'id' column to be a sequential range starting from 1\n",
    "df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "# Step 4: Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV file saved as '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the newly uploaded files\n",
    "new_reference_path = 'process-compressed/reference.csv'\n",
    "new_shuffled_path = 'process-compressed/shuffled.csv'\n",
    "\n",
    "new_reference_df = pd.read_csv(new_reference_path)\n",
    "new_shuffled_df = pd.read_csv(new_shuffled_path)\n",
    "\n",
    "# Ensure IDs are 1-based in the shuffled DataFrame\n",
    "new_shuffled_df['id'] = new_shuffled_df.index + 1\n",
    "\n",
    "# Add a unique identifier to each row to maintain original indices\n",
    "new_reference_df['ref_unique_id'] = new_reference_df.index\n",
    "new_shuffled_df['shuffled_unique_id'] = new_shuffled_df.index\n",
    "\n",
    "# Create a dictionary from the shuffled DataFrame for quick lookups\n",
    "shuffled_dict = {}\n",
    "for _, row in new_shuffled_df.iterrows():\n",
    "    key = (row['x'], row['y'], row['z'])\n",
    "    if key not in shuffled_dict:\n",
    "        shuffled_dict[key] = []\n",
    "    shuffled_dict[key].append((row['id'], row['shuffled_unique_id']))\n",
    "\n",
    "# Iterate through the reference DataFrame and match rows using the dictionary\n",
    "matched_rows = []\n",
    "used_indices = set()\n",
    "\n",
    "for _, ref_row in new_reference_df.iterrows():\n",
    "    key = (ref_row['x'], ref_row['y'], ref_row['z'])\n",
    "    if key in shuffled_dict:\n",
    "        for shuf_row in shuffled_dict[key]:\n",
    "            if shuf_row[1] not in used_indices:\n",
    "                matched_rows.append({\n",
    "                    'id': shuf_row[0],\n",
    "                    'x': ref_row['x'],\n",
    "                    'y': ref_row['y'],\n",
    "                    'z': ref_row['z'],\n",
    "                    'original_index_shuffled': shuf_row[1]\n",
    "                })\n",
    "                used_indices.add(shuf_row[1])\n",
    "                break\n",
    "        else:\n",
    "            matched_rows.append({\n",
    "                'id': None,\n",
    "                'x': ref_row['x'],\n",
    "                'y': ref_row['y'],\n",
    "                'z': ref_row['z'],\n",
    "                'original_index_shuffled': None\n",
    "            })\n",
    "    else:\n",
    "        matched_rows.append({\n",
    "            'id': None,\n",
    "            'x': ref_row['x'],\n",
    "            'y': ref_row['y'],\n",
    "            'z': ref_row['z'],\n",
    "            'original_index_shuffled': None\n",
    "        })\n",
    "\n",
    "# Create the result DataFrame\n",
    "result_df = pd.DataFrame(matched_rows)\n",
    "\n",
    "# Preserve original IDs without changing them to 1-based again\n",
    "result_df['id'] = result_df['id'].fillna(method='ffill')\n",
    "\n",
    "# Check if the number of rows in the corrected file matches the reference file\n",
    "rows_match_final_correct = len(result_df) == len(new_reference_df)\n",
    "\n",
    "output_path_final_correct = 'process-compressed/corrected_reference.csv'\n",
    "result_df.to_csv(output_path_final_correct, index=False)\n",
    "\n",
    "rows_match_final_correct, output_path_final_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save the ground truth displacements x,y, and z in separate file, and the the Node Indices*\n",
    "\n",
    "*Based on the Node indices, the order of the predicted displacements will be corrected*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_correct_df = pd.read_csv(output_path_final_correct)\n",
    "\n",
    "# Extract the x, y, z columns\n",
    "xyz_df = final_correct_df[['x', 'y', 'z']]\n",
    "\n",
    "indice_df = final_correct_df[['id']]\n",
    "\n",
    "# Save to a new CSV file without header and index\n",
    "output_path_xyz = 'process-compressed/xyz_gt.csv'\n",
    "xyz_df.to_csv(output_path_xyz, index=False, header=False)\n",
    "\n",
    "indice_path = 'process-compressed/indice.csv'\n",
    "indice_df.to_csv(indice_path, index=False, header=True)\n",
    "\n",
    "\n",
    "output_path_xyz\n",
    "'''\n",
    "\n",
    "'''# Load the indice and prediction files\n",
    "prediction_path = 'process-compressed/prediction.csv'\n",
    "\n",
    "indice_df = pd.read_csv(indice_path)\n",
    "prediction_df = pd.read_csv(prediction_path)\n",
    "\n",
    "# Merge the prediction DataFrame with the indice DataFrame on the id column\n",
    "merged_df = prediction_df.merge(indice_df[['id']], on='id')\n",
    "\n",
    "# Sort the merged DataFrame based on the order of the id column in the indice DataFrame\n",
    "sorted_df = merged_df.set_index('id').loc[indice_df['id']].reset_index()\n",
    "\n",
    "# Extract the x, y, z columns\n",
    "xyz_df = sorted_df[['x', 'y', 'z']]\n",
    "\n",
    "# Save the reordered result to a new CSV file without header and index\n",
    "output_path_xyz_reordered = 'process-compressed/xyz_pred.csv'\n",
    "xyz_df.to_csv(output_path_xyz_reordered, index=False, header=False)\n",
    "\n",
    "output_path_xyz_reordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add the displacements to the uncomressed mesh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reordered predicted displacements\n",
    "diff_csv = 'process-compressed/xyz_pred.csv'\n",
    "\n",
    "df_diff = pd.read_csv(diff_csv, header=None)\n",
    "\n",
    "# Load the existing unstructured grid\n",
    "vtk_filename = 'GNNs-BreastCompression/uncompressed_nrrd/volmesh3.vtk'\n",
    "reader = vtk.vtkUnstructuredGridReader()\n",
    "reader.SetFileName(vtk_filename)\n",
    "reader.Update()\n",
    "\n",
    "unstructured_grid = reader.GetOutput()\n",
    "\n",
    "# Step 3: Validate the displacement data and grid size\n",
    "num_points = unstructured_grid.GetNumberOfPoints()\n",
    "\n",
    "# Step 4: Create the displacement field data array\n",
    "displacement_array = vtk.vtkFloatArray()\n",
    "displacement_array.SetName(\"displacements\")\n",
    "displacement_array.SetNumberOfComponents(3)  # x, y, z components\n",
    "displacement_array.SetNumberOfTuples(num_points)\n",
    "\n",
    "# Step 5: Populate the vtkFloatArray with data from displacement_df\n",
    "for i in range(num_points):\n",
    "    displacement = df_diff.iloc[i].values\n",
    "    displacement_array.SetTuple(i, displacement)\n",
    "\n",
    "# Step 6: Add the displacement data to the unstructured grid's point data\n",
    "unstructured_grid.GetPointData().AddArray(displacement_array)\n",
    "unstructured_grid.GetPointData().SetActiveVectors(\"displacements\")\n",
    "\n",
    "# Save the updated unstructured grid to a new VTK file\n",
    "output_vtk = 'process-compressed/volmesh3_physgnn.vtk'\n",
    "writer = vtk.vtkUnstructuredGridWriter()\n",
    "writer.SetFileName(output_vtk)\n",
    "writer.SetInputData(unstructured_grid)\n",
    "writer.Write()\n",
    "\n",
    "print(f\"Updated VTK file saved to '{output_vtk}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Get the compressed phantom from the output of Niftysim Docker container*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_number = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python FEA-simualtions/extract_compressed_mesh.py process-compressed/volmesh{case_number}_physgnn.vtk process-compressed/volmesh{case_number}_physgnn_compressed.vtk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reconstruct the NRRD image from the compressed phantom mesh of PhysGNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Update the -v path !!'''\n",
    "\n",
    "# Run Docker container with GPU support, mounting a volume, and using an interactive terminal\n",
    "!docker run --name reconstruct_container --gpus all -v /home/hadeel/GNNs-BreastCompression:/home/data -d reconstruct-image:latest tail -f /dev/null\n",
    "\n",
    "!docker ps\n",
    "\n",
    "!docker exec reconstruct_container /bin/bash -c \"cd /home/ReconstructImage/release && ./ReconstructImage /home/data/uncompressed_nrrd/volmesh{case_number}.vtk /home/data/process-compressed/volmesh{case_number}_physgnn_compressed.vtk /home/data/uncompressed_nrrd/UncompressedBreast{case_number}_resampled.nrrd /home/data/process-compressed/compressedPhantom{case_number}_physgnn.nrrd\"\n",
    "\n",
    "!docker stop reconstruct_container\n",
    "!docker rm reconstruct_container"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
