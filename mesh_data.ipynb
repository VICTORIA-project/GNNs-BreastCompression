{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generator from the Uncompressed mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Working Directory to the Notebook's Path\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "notebook_path = Path().resolve()\n",
    "os.chdir(notebook_path)\n",
    "\n",
    "# Confirm the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNNs-BreastCompression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the mesh file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding Box Coordinates (xmin, xmax, ymin, ymax, zmin, zmax): (31.819000244140625, 153.58299255371094, 39.951900482177734, 166.66000366210938, -0.06298580020666122, 93.52210235595703)\n"
     ]
    }
   ],
   "source": [
    "import vtk\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter \n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "\n",
    "file_path = \"uncompressed_nrrd/volmesh3.vtk\" # be carefull to the file\n",
    "reader = vtk.vtkUnstructuredGridReader()\n",
    "reader.SetFileName(file_path)\n",
    "reader.Update()\n",
    "\n",
    "unstructuredGrid = reader.GetOutput()\n",
    "points = unstructuredGrid.GetPoints()  # points (coordinates) of the grid\n",
    "cells = unstructuredGrid.GetCells()  # cells of the grid\n",
    "cell_types = unstructuredGrid.GetCellTypesArray()  # cell types of the grid\n",
    "cell_data = unstructuredGrid.GetCellData()  # cell data associated with the grid\n",
    "point_data = unstructuredGrid.GetPointData()  # point data associated with the grid\n",
    "# Bounding Box of the grid:\n",
    "bbox = points.GetBounds()  # Return a pointer to the geometry bounding box in the form (xmin,xmax, ymin,ymax, zmin,zmax).\n",
    "print(\"Bounding Box Coordinates (xmin, xmax, ymin, ymax, zmin, zmax):\", bbox)\n",
    "num_cells = unstructuredGrid.GetNumberOfCells()\n",
    "num_points = unstructuredGrid.GetNumberOfPoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements and their nodes indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'elements.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "def get_tetrahedron_elements(unstructuredGrid):\n",
    "    \"\"\"\n",
    "    Retrieve tetrahedron elements and their point indices (1-based).\n",
    "    \n",
    "    Parameters:\n",
    "    unstructuredGrid (vtkUnstructuredGrid): The unstructured grid to retrieve elements from.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of lists, where each inner list contains the 1-based point indices of a tetrahedron.\n",
    "    \"\"\"\n",
    "    tetrahedron_elements = []\n",
    "\n",
    "    # Loop through each cell in the unstructured grid\n",
    "    num_cells = unstructuredGrid.GetNumberOfCells()\n",
    "    for cell_id in range(num_cells):\n",
    "        cell = unstructuredGrid.GetCell(cell_id)\n",
    "\n",
    "        # Check if the cell is a tetrahedron\n",
    "        if cell.GetCellType() == vtk.VTK_TETRA:\n",
    "            point_ids = cell.GetPointIds()\n",
    "\n",
    "            # Convert point indices to 1-based\n",
    "            point_indices = [point_ids.GetId(i) + 1 for i in range(point_ids.GetNumberOfIds())]\n",
    "\n",
    "            # Add the point indices to the list\n",
    "            tetrahedron_elements.append(point_indices)\n",
    "\n",
    "    return tetrahedron_elements\n",
    "\n",
    "# Retrieve tetrahedron elements (only point indices)\n",
    "tetrahedron_elements = get_tetrahedron_elements(unstructuredGrid)\n",
    "\n",
    "# Save to CSV (no header, only point indices)\n",
    "with open(\"Data_Generator/input/elements.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write each tetrahedron's point indices\n",
    "    for point_indices in tetrahedron_elements:\n",
    "        csv_writer.writerow(point_indices)\n",
    "\n",
    "print(\"CSV file 'elements.csv' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'element_ID.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of cells in the unstructured grid\n",
    "num_cells = unstructuredGrid.GetNumberOfCells()\n",
    "\n",
    "# Retrieve the material scalar data for the cells\n",
    "material_scalars = unstructuredGrid.GetCellData().GetScalars(\"materials\")\n",
    "\n",
    "# Prepare a list for the material IDs\n",
    "material_ids = []\n",
    "\n",
    "# Loop through each cell to get the material ID\n",
    "for cell_id in range(num_cells):\n",
    "    material_id = material_scalars.GetTuple1(cell_id)  # Get material ID for the cell\n",
    "    material_ids.append(material_id)\n",
    "\n",
    "# Save to CSV (no header, only material IDs)\n",
    "with open(\"Data_Generator/input/element_ID.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write each material ID to the CSV file\n",
    "    for material_id in material_ids:\n",
    "        csv_writer.writerow([int(material_id)])\n",
    "\n",
    "print(\"CSV file 'element_ID.csv' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes Coordinates - xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'xyz.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of points (nodes) in the unstructured grid\n",
    "num_points = unstructuredGrid.GetNumberOfPoints()\n",
    "\n",
    "# List to store the point coordinates\n",
    "node_coordinates = []\n",
    "\n",
    "# Loop through each point to get its coordinates\n",
    "for point_id in range(num_points):\n",
    "    point_coords = unstructuredGrid.GetPoint(point_id)\n",
    "    \n",
    "    # Append the coordinates (x, y, z) to the list (excluding the point ID)\n",
    "    node_coordinates.append(point_coords)\n",
    "\n",
    "# Write the data to a CSV file (no header, only x, y, z coordinates)\n",
    "with open(\"Data_Generator/input/xyz.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write each point's coordinates (x, y, z) to the CSV file\n",
    "    for coords in node_coordinates:\n",
    "        csv_writer.writerow(coords)\n",
    "\n",
    "print(\"CSV file 'xyz.csv' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rigid ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'bcSupportList.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the boundary condition scalar from point data\n",
    "boundary_scalars = unstructuredGrid.GetPointData().GetScalars(\"boundaryConditions\")\n",
    "\n",
    "# List to store the point boundary conditions\n",
    "node_boundary_conditions = []\n",
    "\n",
    "# Loop through each point to get its boundary condition\n",
    "for point_id in range(num_points):\n",
    "    # Get the boundary condition scalar for this point\n",
    "    rigid_id = boundary_scalars.GetTuple1(point_id)  # Assumes it's a single-component scalar\n",
    "\n",
    "    # Convert point ID to 1-based\n",
    "    one_based_point_id = point_id + 1\n",
    "\n",
    "    # Append the point ID and boundary condition to the list\n",
    "    node_boundary_conditions.append([one_based_point_id, rigid_id])\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(\"Data_Generator/input/bcSupportList.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    # Write header\n",
    "    csv_writer.writerow([\"Node ID\", \"Rigid_ID\"])  # Custom header for boundary conditions\n",
    "\n",
    "    # Write each point's ID and boundary condition\n",
    "    for node in node_boundary_conditions:\n",
    "        csv_writer.writerow([int(node[0]), int(node[1])])\n",
    "\n",
    "print(\"CSV file 'bcSupportList.csv' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find surface nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normals saved to input/normals.csv\n"
     ]
    }
   ],
   "source": [
    "def compute_normals(unstructured_grid):\n",
    "    # Compute gradients (which can be used to derive normals)\n",
    "    gradient_filter = vtk.vtkCellDerivatives()\n",
    "    gradient_filter.SetInputData(unstructured_grid)\n",
    "    gradient_filter.SetVectorModeToComputeGradient()\n",
    "    gradient_filter.Update()\n",
    "\n",
    "    # Access the computed gradients\n",
    "    gradients = gradient_filter.GetOutput().GetCellData().GetVectors()\n",
    "    if gradients is None:\n",
    "        raise RuntimeError(\"Gradients not found in the dataset.\")\n",
    "\n",
    "    # Average the cell gradients to point gradients\n",
    "    point_gradients = vtk.vtkFloatArray()\n",
    "    point_gradients.SetNumberOfComponents(3)\n",
    "    point_gradients.SetName(\"Gradients\")\n",
    "\n",
    "    num_points = unstructured_grid.GetNumberOfPoints()\n",
    "    for i in range(num_points):\n",
    "        point_gradient = [0.0, 0.0, 0.0]\n",
    "        cell_ids = vtk.vtkIdList()\n",
    "        unstructured_grid.GetPointCells(i, cell_ids)\n",
    "        num_cells = cell_ids.GetNumberOfIds()\n",
    "\n",
    "        for j in range(num_cells):\n",
    "            cell_id = cell_ids.GetId(j)\n",
    "            cell_gradient = gradients.GetTuple(cell_id)\n",
    "            point_gradient = [sum(x) for x in zip(point_gradient, cell_gradient)]\n",
    "\n",
    "        point_gradient = [x / num_cells for x in point_gradient]\n",
    "        point_gradients.InsertNextTuple(point_gradient)\n",
    "\n",
    "    unstructured_grid.GetPointData().SetVectors(point_gradients)\n",
    "    return unstructured_grid\n",
    "\n",
    "def extract_normals(unstructured_grid):\n",
    "    # Extract gradients from unstructured grid and normalize to get normals\n",
    "    gradients = unstructured_grid.GetPointData().GetVectors()\n",
    "    if gradients is None:\n",
    "        raise RuntimeError(\"Gradients not found in the dataset.\")\n",
    "\n",
    "    # Convert VTK array to a numpy array\n",
    "    num_points = unstructured_grid.GetNumberOfPoints()\n",
    "    normals_array = np.zeros((num_points, 3))\n",
    "    for i in range(num_points):\n",
    "        gradient = np.array(gradients.GetTuple(i))\n",
    "        norm = np.linalg.norm(gradient)\n",
    "        if norm != 0:\n",
    "            normal = gradient / norm\n",
    "        else:\n",
    "            normal = gradient\n",
    "        normals_array[i] = normal\n",
    "\n",
    "    return normals_array\n",
    "\n",
    "def save_normals_to_csv(filename, normals_array):\n",
    "    # Save the node indices and their respective normals to a CSV file\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Node Index', 'Normal X', 'Normal Y', 'Normal Z'])\n",
    "        for i, normal in enumerate(normals_array):\n",
    "            writer.writerow([i + 1, normal[0], normal[1], normal[2]])\n",
    "\n",
    "def main(unstructured_grid, output_csv_filename):\n",
    "\n",
    "    # Compute normals\n",
    "    unstructured_grid_with_gradients = compute_normals(unstructured_grid)\n",
    "    \n",
    "    # Extract normals\n",
    "    normals_array = extract_normals(unstructured_grid_with_gradients)\n",
    "    \n",
    "    # # Save normals to CSV\n",
    "    save_normals_to_csv(output_csv_filename, normals_array)\n",
    "\n",
    "# Example usage\n",
    "output_csv_filename = 'Data_Generator/input/normals.csv'\n",
    "main(unstructuredGrid, output_csv_filename)\n",
    "print(f\"Normals saved to {output_csv_filename}\")\n",
    "'''\n",
    "#########################################################\n",
    "# Extract node indices 1-based with non-zero normals and nodes with non-zero normals\n",
    "'''\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('Data_Generator/input/normals.csv')\n",
    "\n",
    "# Filter nodes with non-zero normals\n",
    "non_zero_normals = df[(df['Normal X'] != 0) | (df['Normal Y'] != 0) | (df['Normal Z'] != 0)]\n",
    "\n",
    "# Extract node indices with non-zero normals\n",
    "node_indices_non_zero_normals = non_zero_normals['Node Index']\n",
    "\n",
    "node_indices_non_zero_normals_df = node_indices_non_zero_normals.to_frame(name=\"Node Index\")\n",
    "\n",
    "# Load the support list\n",
    "bc_support_list = pd.read_csv(\"Data_Generator/input/bcSupportList.csv\")\n",
    "\n",
    "\n",
    "# Check if each index is in the support list and has Rigid_ID equal to 1\n",
    "for index, row in node_indices_non_zero_normals_df.iterrows():\n",
    "    node_id = row[\"Node Index\"]\n",
    "    if node_id in bc_support_list[\"Node ID\"].values:\n",
    "        rigid_id = bc_support_list.loc[bc_support_list[\"Node ID\"] == node_id, \"Rigid_ID\"].values[0]\n",
    "        if rigid_id == 1:\n",
    "            # Delete the row\n",
    "            node_indices_non_zero_normals_df.drop(index, inplace=True)\n",
    "\n",
    "# Save the filtered dataframe to a new CSV file\n",
    "node_indices_non_zero_normals_df.to_csv(\"Data_Generator/input/bcPrescribeList.csv\", index=False)\n",
    "\n",
    "#convert from 1_based to 0-based\n",
    "df = pd.read_csv(\"Data_Generator/input/bcPrescribeList.csv\")\n",
    "\n",
    "# Subtract 1 from the values in the first column to convert from 1-based to 0-based indices\n",
    "df[\"Node Index\"] -= 1\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv(\"Data_Generator/input/bcPrescribeList_0_based.csv\", index=False)\n",
    "\n",
    "\n",
    "# Update bcSupportList.csv (remove 'Node ID' column and header)\n",
    "bc_support_list = pd.read_csv(\"Data_Generator/input/bcSupportList.csv\")\n",
    "bc_support_list.drop(columns=[\"Node ID\"], inplace=True)\n",
    "bc_support_list.to_csv(\"Data_Generator/input/bcSupportList.csv\", index=False, header=False)  # Remove header\n",
    "\n",
    "# Update bcPrescribeList_0_based.csv (remove 'Node ID' column and header)\n",
    "bc_prescribe_list = pd.read_csv(\"Data_Generator/input/bcPrescribeList_0_based.csv\")#, header=True)\n",
    "bc_prescribe_list.to_csv(\"Data_Generator/input/bcPrescribeList_0_based.csv\", index=False, header=False)  # Remove header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching DataFrame:\n",
      "      Node Index  Normal X  Normal Y  Normal Z\n",
      "0             85 -0.079704 -0.066261 -0.994614\n",
      "1             89  0.389122 -0.392274 -0.833490\n",
      "2            102  0.135919  0.069030 -0.988312\n",
      "3            104  0.172890 -0.005404 -0.984926\n",
      "4            108  0.001642  0.233260 -0.972413\n",
      "...          ...       ...       ...       ...\n",
      "1124       17534  0.008986  0.056693 -0.998351\n",
      "1125       17540 -0.027402  0.046397 -0.998547\n",
      "1126       17543 -0.027241 -0.059276 -0.997870\n",
      "1127       17564 -0.094505  0.072314 -0.992895\n",
      "1128       17577  0.149883 -0.104682 -0.983146\n",
      "\n",
      "[1129 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# get only the columns for x,y,z normal directions\n",
    "\n",
    "# Merge the two dataframes based on 'Node Index' (intersection of matching indices)\n",
    "matching_df = pd.merge(non_zero_normals, node_indices_non_zero_normals_df, on='Node Index')\n",
    "\n",
    "# Check if merge is successful\n",
    "print(f\"Matching DataFrame:\\n{matching_df}\")\n",
    "\n",
    "# Extract the second, third, and fourth columns\n",
    "directions = matching_df.iloc[:, 1:4]\n",
    "\n",
    "# Extract each column\n",
    "x_direction = directions.iloc[:, 0]\n",
    "y_direction = directions.iloc[:, 1]\n",
    "z_direction = directions.iloc[:, 2]\n",
    "\n",
    "# Save each column as a separate CSV file\n",
    "x_direction.to_csv('Data_Generator/input/x_direction.csv', index=False, header=False)\n",
    "y_direction.to_csv('Data_Generator/input/y_direction.csv', index=False, header=False)\n",
    "z_direction.to_csv('Data_Generator/input/z_direction.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the 10 distinct batches of 3 random directions along with the normal direction: a, b, c, ..., j\n",
    "```\n",
    "│\n",
    "├── Hold-out\n",
    "│   └── dataset\n",
    "│       ├── a\n",
    "│       │   ├── force_dir_x_a.csv\n",
    "│       │   ├── force_dir_y_a.csv\n",
    "│       │   └── force_dir_z_a.csv\n",
    "│       ├── b\n",
    "│       │   ├── force_dir_x_b.csv\n",
    "│       │   ├── force_dir_y_b.csv\n",
    "│       │   └── force_dir_z_b.csv\n",
    "│       ├── c\n",
    "│       │   └── ...\n",
    "│       └── j\n",
    "│           ├── force_dir_x_j.csv\n",
    "│           ├── force_dir_y_j.csv\n",
    "│           └── force_dir_z_j.csv\n",
    "│\n",
    "└── LODO\n",
    "    ├── dataset\n",
    "    │   ├── a\n",
    "    │   │   ├── force_dir_x_a.csv\n",
    "    │   │   ├── force_dir_y_a.csv\n",
    "    │   │   └── force_dir_z_a.csv\n",
    "    │   ├── b\n",
    "    │   │   ├── force_dir_x_b.csv\n",
    "    │   │   ├── force_dir_y_b.csv\n",
    "    │   │   └── force_dir_z_b.csv\n",
    "    │   ├── c\n",
    "    │   │   └── ...\n",
    "    │   └── j\n",
    "    │       ├── force_dir_x_j.csv\n",
    "    │       ├── force_dir_y_j.csv\n",
    "    │       └── force_dir_z_j.csv\n",
    "    │\n",
    "    └── final_step\n",
    "        ├── a\n",
    "        │   ├── force_dir_x_a.csv (one direction only)\n",
    "        │   ├── force_dir_y_a.csv (one direction only)\n",
    "        │   └── force_dir_z_a.csv (one direction only)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended new directions to x_direction.csv, y_direction.csv, and z_direction.csv.\n"
     ]
    }
   ],
   "source": [
    "def sample_hemisphere_x(n):\n",
    "    \"\"\"\n",
    "    Generate `n` random directions from a hemisphere with radius 1 along the positive x-axis.\n",
    "\n",
    "    :param n: Number of samples to generate.\n",
    "    :return: Three separate lists containing x, y, and z components of the directions.\n",
    "    \"\"\"\n",
    "    theta = np.random.uniform(0, np.pi / 2, n)  # Polar angle: 0 to pi/2 for hemisphere along x-axis\n",
    "    phi = np.random.uniform(0, 2 * np.pi, n)  # Azimuthal angle: 0 to 2pi for full rotation\n",
    "    \n",
    "    x = np.cos(theta)\n",
    "    y = np.sin(theta) * np.cos(phi)\n",
    "    z = np.sin(theta) * np.sin(phi)\n",
    "    \n",
    "    return x.tolist(), y.tolist(), z.tolist()\n",
    "\n",
    "def sample_hemisphere_y(n):\n",
    "    \"\"\"\n",
    "    Generate `n` random directions from a hemisphere with radius 1 along the positive y-axis.\n",
    "\n",
    "    :param n: Number of samples to generate.\n",
    "    :return: Three separate lists containing x, y, and z components of the directions.\n",
    "    \"\"\"\n",
    "    theta = np.random.uniform(0, np.pi / 2, n)  # Polar angle: 0 to pi/2 for hemisphere along y-axis\n",
    "    phi = np.random.uniform(0, 2 * np.pi, n)  # Azimuthal angle: 0 to 2pi for full rotation\n",
    "    \n",
    "    x = np.sin(theta) * np.cos(phi)\n",
    "    y = np.cos(theta)\n",
    "    z = np.sin(theta) * np.sin(phi)\n",
    "    \n",
    "    return x.tolist(), y.tolist(), z.tolist()\n",
    "\n",
    "def sample_hemisphere_z(n):\n",
    "    \"\"\"\n",
    "    Generate `n` random directions from a hemisphere with radius 1 along the positive z-axis.\n",
    "\n",
    "    :param n: Number of samples to generate.\n",
    "    :return: Three separate lists containing x, y, and z components of the directions.\n",
    "    \"\"\"\n",
    "    theta = np.random.uniform(0, np.pi / 2, n)  # Polar angle: 0 to pi/2 for hemisphere along z-axis\n",
    "    phi = np.random.uniform(0, 2 * np.pi, n)  # Azimuthal angle: 0 to 2pi for full rotation\n",
    "    \n",
    "    x = np.sin(theta) * np.cos(phi)\n",
    "    y = np.sin(theta) * np.sin(phi)\n",
    "    z = np.cos(theta)\n",
    "    \n",
    "    return x.tolist(), y.tolist(), z.tolist()\n",
    "\n",
    "# Read existing data\n",
    "existing_data_x = pd.read_csv('Data_Generator/input/x_direction.csv', header=None)\n",
    "existing_data_y = pd.read_csv('Data_Generator/input/y_direction.csv', header=None)\n",
    "existing_data_z = pd.read_csv('Data_Generator/input/z_direction.csv', header=None)\n",
    "\n",
    "# Determine the number of samples to generate based on the number of rows in the existing data\n",
    "n_samples = existing_data_x.shape[0]\n",
    "\n",
    "# Initialize lists to store new directions\n",
    "new_x_columns = []\n",
    "new_y_columns = []\n",
    "new_z_columns = []\n",
    "\n",
    "# Generate and append new directions\n",
    "for _ in range(n_samples):\n",
    "    x_list_x, y_list_x, z_list_x = sample_hemisphere_x(1)\n",
    "    x_list_y, y_list_y, z_list_y = sample_hemisphere_y(1)\n",
    "    x_list_z, y_list_z, z_list_z = sample_hemisphere_z(1)\n",
    "    \n",
    "    new_x_columns.append([x_list_x[0], y_list_x[0], z_list_x[0]])\n",
    "    new_y_columns.append([x_list_y[0], y_list_y[0], z_list_y[0]])\n",
    "    new_z_columns.append([x_list_z[0], y_list_z[0], z_list_z[0]])\n",
    "\n",
    "# Convert the new columns to DataFrames\n",
    "new_x_df = pd.DataFrame(new_x_columns)\n",
    "new_y_df = pd.DataFrame(new_y_columns)\n",
    "new_z_df = pd.DataFrame(new_z_columns)\n",
    "\n",
    "# Concatenate the existing data with the new directions\n",
    "combined_data_x = pd.concat([existing_data_x, new_x_df], axis=1)\n",
    "combined_data_y = pd.concat([existing_data_y, new_y_df], axis=1)\n",
    "combined_data_z = pd.concat([existing_data_z, new_z_df], axis=1)\n",
    "\n",
    "import string\n",
    "letters = list(string.ascii_lowercase[:10])  # This will give you 'a' to 'j'\n",
    "\n",
    "for i in letters:\n",
    "    # Ensure that directories exist for Hold-out\n",
    "    holdout_dir = f'Hold-out/dataset/{i}'\n",
    "    os.makedirs(holdout_dir, exist_ok=True)\n",
    "\n",
    "    # Ensure that directories exist for LODO\n",
    "    lodo_dir = f'LODO/dataset/{i}'\n",
    "    os.makedirs(lodo_dir, exist_ok=True)\n",
    "\n",
    "    # Save combined data for Hold-out\n",
    "    combined_data_x.to_csv(f'{holdout_dir}/force_dir_x_{i}.csv', header=False, index=False)\n",
    "    combined_data_y.to_csv(f'{holdout_dir}/force_dir_y_{i}.csv', header=False, index=False)\n",
    "    combined_data_z.to_csv(f'{holdout_dir}/force_dir_z_{i}.csv', header=False, index=False)\n",
    "\n",
    "    # Save combined data for LODO\n",
    "    combined_data_x.to_csv(f'{lodo_dir}/force_dir_x_{i}.csv', header=False, index=False)\n",
    "    combined_data_y.to_csv(f'{lodo_dir}/force_dir_y_{i}.csv', header=False, index=False)\n",
    "    combined_data_z.to_csv(f'{lodo_dir}/force_dir_z_{i}.csv', header=False, index=False)\n",
    "\n",
    "\n",
    "print(f\"Appended new directions to x_direction.csv, y_direction.csv, and z_direction.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random directions saved to CSV files in /home/hadeel/backup/rackmic/LODO/final_step/a/\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(existing_data_x)\n",
    "\n",
    "# Step 2: Generate random x, y, z directions with num_rows\n",
    "random_directions = np.random.uniform(-1, 1, (num_rows, 3))  # Generate (num_rows, 3) array with values between -1 and 1\n",
    "\n",
    "# Step 3: Normalize each row to create unit vectors\n",
    "norms = np.linalg.norm(random_directions, axis=1, keepdims=True)  # Calculate the magnitude for each row\n",
    "norms[norms == 0] = 1  # Replace zeros to avoid division by zero, set to default direction\n",
    "normalized_directions = random_directions / norms  # Normalize each row\n",
    "\n",
    "# Step 4: Split the normalized directions into x, y, z components\n",
    "random_x = normalized_directions[:, 0]\n",
    "random_y = normalized_directions[:, 1]\n",
    "random_z = normalized_directions[:, 2]\n",
    "\n",
    "# Step 5: Save the components to separate CSV files\n",
    "output_folder = \"LODO/final_step/a\" \n",
    "os.makedirs(output_folder, exist_ok=True)  \n",
    "pd.DataFrame(random_x).to_csv(f'{output_folder}/force_dir_x_a.csv', header=False, index=False)\n",
    "pd.DataFrame(random_y).to_csv(f'{output_folder}/force_dir_y_a.csv', header=False, index=False)\n",
    "pd.DataFrame(random_z).to_csv(f'{output_folder}/force_dir_z_a.csv', header=False, index=False)\n",
    "\n",
    "print(f\"Random directions saved to CSV files in {output_folder}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mesh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
